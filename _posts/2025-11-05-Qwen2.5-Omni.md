---
title: "Qwen2.5-Omni"
date: 2025-11-05 15:18
category: "논문-리뷰"
tag: [LLM, Video, Text, Image, Audio, TTS, Streaming, Real-Time]
published: true

toc: true
toc_sticky: true
use_math: true
---
> [Qwen2.5-Omni Technical Report](https://arxiv.org/pdf/2503.20215)
> [[Github](https://github.com/QwenLM/Qwen2.5-Omni)]
> [[Hugging Face](https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo)]
> [[블로그](https://qwen.ai/blog?id=9ef8b30f398c303da67ab622204b07d6c74af9cd&from=research.research-list)] 

<figure class="centered-figure">
  <img src="../assets/images/Qwen2.5-omni/0001.jpeg"  style="width: 80%;">
  <figcaption class="caption">텍스트, 오디오, 이미지, 영상을 입력받아 실시간으로 텍스트와 음성 응답을 생성하는 멀티모달 모델인 Qwen2.5-Omni</figcaption>
</figure>

# 📌 이 논문을 읽은 이유
<!-- 줄글로 작성 (내 동기, 관심사, 맥락 설명)
예시: 
이 논문은 제가 진행 중인 [프로젝트/연구 분야]와 관련된 문제를 다루고 있어서 읽게 되었습니다. 
특히 기존 방법에서 느꼈던 [문제점]을 개선한 접근이라는 점에서 흥미를 가졌습니다.
-->
Qwen3-Omni를 이해하기 위해 읽게 되었다.

기존에도 텍스트-이미지, 텍스트-오디오 등 두 가지 모달을 함께 사용하는 모델들은 많았지만, 네 가지 모달을 동시에 사용하면서도 높은 성능을 보이는 모델은 드물었다.

오디오 AI 연구를 진행하면서 언어적 특성과 음성적 특성을 함께 이해할 수 있는 모델의 필요성을 자주 느꼈다. 음성에는 텍스트적인 요소와 반언어적인 요소가 함께 존재하며, 그중 하나만 사용했을 때는 성능적 한계가 분명해 보였기 때문이다.


이 논문은 LLM 기반으로 하면서 음성부터 영상까지 지원하고, 실시간으로 응답을 생성하는 점이 특히 인상 깊었다.


# 💡 논문의 주요 내용 간단 요약
<!-- 줄글로 작성 (내 해석 중심)
예시: 
이 논문은 [문제 정의]를 해결하기 위해 [핵심 아이디어]를 제안합니다. 
기존 방법과 비교했을 때, [차별점/장점]이 두드러지며 [어떤 상황]에서 특히 효과적입니다.
-->

<b>Qwen2.5-Omni는?</b>
- 텍스트, 오디오, 이미지, 영상을 입력 받아, 이에 대한 응답을 <b>실시간 텍스트와 음성으로 생성하는 멀티모달 모델</b>
- 뇌처럼 생각하고, 입처럼 대답하는 <span style='background-color:#fff5b1'>Thinker-Talker 구조</span> 사용
- 오디오와 영상의 시계열 요소를 고려하기 위해, <span style='background-color:#fff5b1'>TMRoPE(Time-aligned Multimodal RoPE)</span>라는 새로운 positional embedding 방법 제안



# 📚 핵심 아이디어와 주요 기법 정리
<!-- Bullet 형태로 작성 (논문 내용 요약 / 정리)
예시:
- 논문은 [기법/구성요소1]을 통해 [문제 A]를 해결함.
- [기법/구성요소2]는 [어떤 점]에서 기존보다 나은 성능을 보임.
- [기법/구성요소3]는 추가적으로 [효과/장점]을 제공함.
-->
## Thinker-Talker 구조
<figure class="centered-figure">
  <img src="../assets/images/Qwen2.5-omni/0002.jpeg"  style="width: 80%;">
  <figcaption class="caption">Thinker-Talker 구조</figcaption>
</figure>

<b>Thinker</b>
- 사람의 "뇌"처럼 입력 값을 이해 $\rightarrow$ high-level representation과 관련된 텍스트 생성
- 입력값은 텍스트, 오디오, 이미지, 영상을 지원
- 오디오 encoder + 이미지 encoder + Transformer decoder 구조

<b>Talker</b>
- 사람의 "입"처럼 음성을 생성하는 부분
- Thinker의 high-level representation과 텍스트 token을 스트리밍 방식으로 입력받고 $\rightarrow$ 이산적인 음성 token을 출력
- 이중 트랙(dual-track) autoregressive Transformer decoder 구조


## 어떻게 다양한 모달을 입력 받을까? Perceivation!
<figure class="centered-figure">
  <img src="../assets/images/Qwen2.5-omni/0003.jpeg"  style="width: 80%;">
  <figcaption class="caption">TMRoPE 값 예시. 해당 값이 각 representation에 더해진다.</figcaption>
</figure>

<b>다양한 입력값</b>
- <b>텍스트</b>: byte-level byte-pair encoding하는 Qwen tokenizer 사용
- <b>오디오</b>: 16 kHz로 샘플링 $\rightarrow$ 128-channel Mel-Spectrogram (window size 25ms, hop size 10ms) $\rightarrow$ Qwen2-Audio encoder (각 frame이 원래 음성의 40ms와 대략적으로 일치하게 바꿈) 사용
- <b>비전</b>: 이미지 & 영상 $\rightarrow$ ViT 기반인 Qwen2.5-VL 사용. 음성 sampling rate와 맞추기 위해 dynamic frame rate 사용. 이미지는 2개의 동일한 이미지로 취급하여 일관성 유지
<br><br>

<b>TMRoPE(Time-aligned Multimodal RoPE)</b>
- M-RoPE(Multimodal RoPE) + 절대 시간적 위치(absolute temporal position)으로, 멀티모달의 3D 위치적 정보를 인코딩함
- rotary embedding을 3가지 요소로 분해 $\Rightarrow$ temporal, height, width
    - <b>텍스트</b>: 3가지 요소 모두 동일한 positionID 값 사용
    - <b>오디오</b>: 40ms를 기준으로 같은 temporal ID를 사용하며, 3가지 요소에서 동일한 해당 값을 positionID으로 사용
    - <b>이미지</b>: temporal ID는 상수로 변하지 않지만, 이미지의 height와 width에 다른 ID 적용
    - <b>비디오</b>: 음성처럼 40ms를 기준으로 같은 temporal ID를 사용하며, 이미지처럼 height와 width에 다른 ID 적용함
- 멀티모달 입력이 들어올 경우, 각 positional 숫자는 이전의 모달의 마지막 숫자에서 1을 더한 값을 사용함. 즉, 연속된 positional 값을 사용하게 됨.
- positional 값을 할당한 후에는 representation을 재정렬하게 됨. 이를 time-interleaving 방법이라고 칭하기로 함. 
    - 2초마다 자른 비디오와 음성을, visual - 오디오 순서로 번가락하여 배열함

## 생성
<b>텍스트</b>
- Thinker에서 바로 생성됨
- LLM과 동일하게 생성됨

<b>음성</b>
- Thinker에서 생성된 high-level representation + text token embedding $\rightarrow$ Talker가 받아서 사용
    - high-level representation: streaming 방식이기 때문에, 미리 어떤 tone의 음성을 만들어야 알아야하기 때문에 해당 값을 받아옴. 하지만 해당 값은 representational space에서 사전적인 유사도가 높은 값끼리 뭉쳐있음. 음성을 생성할 시에 전혀 다른 단어를 생성할 여지가 있음.
    - text token embedding: 정확한 단어를 사용하기 위해 텍스트 정보를 함께 입력해줌
    - (ex) cat 입력 $\Rightarrow$ high-level representation만 사용했을 때 "kitten"이라는 음성이 생성될 수 있음. 이를 방지하기 위해 text token embedding 사용
- 더 효율적인 speech codec인 <i>qwen-tts-tokenizer</i>
    - 음성에서 중요한 정보를 추출하고, 스트리밍 방식으로 음성으로 decoding이 가능함
    - Talker는 오디오와 텍스트 token을 autoregressive하게 생성하는데, 음성을 생성할 때 word나 timestamp-level 정렬(alignment)가 필요가 없음 $\Rightarrow$ 학습과 추론 단계가 간단해짐


## 스트리밍
<b>Prefilling</b>
- 시간(temporal) 차원을 따라 block-wise attention을 지원
- audio encoder: 전체에 대한 full attention 대신 2초 block 내에서의 attention 수행
- vision encoder: 학습에는 flash attention 사용. 추론에는 2x2 token을 하나의 token으로 바꿔주는 MLP 레이어 사용. patch 크기는 14로 다양한 해상도 지원

<b>스트리밍 Codec 생성</b>
<figure class="centered-figure">
  <img src="../assets/images/Qwen2.5-omni/0004.jpeg"  style="width: 80%;">
  <figcaption class="caption">TMRoPE 값 예시. 해당 값이 각 representation에 더해진다.</figcaption>
</figure>

- 음성 생성 과정: <b>code</b> $-\text{Flow-Matching}\rightarrow$ <b>mel-spectorgram</b> $-\text{BigVGAN}\rightarrow$ <b>waveform</b>
- sliding window block attention으로 오디오 스트리밍 지원
    - 인접한 code를 block으로 만들고 이를 attention maask로 사용
    - 해당 논문에서는 DiT의 reception field를 4 block으로 설정하여, lookback 2개, lookahead 1개 사용
    - 각 code와 관련 block들을 함께 넣어주어 Flow-Matching 방법으로 mel-spectrum 조각을 만들고, 해당 조각을 BigVGAN을 통해 waveform 생성

## Pre-training
1. LLM 고정하고 audio와 vision encoder 각각 학습. 각각 audio-text, image-text 데이터 사용
2. LLM도 같이 학습. 최대 길이는 8192 tokend인 다양한 multimodal 데이터 사용
3. 긴 데이터에 대한 성능 향상을 위해 sequence 길이가 32k인 데이터 사용해서 

## Post-training
<figure class="centered-figure">
  <img src="../assets/images/Qwen2.5-omni/0005.jpeg"  style="width: 80%;">
  <figcaption class="caption">Post-training에 사용하는 데이터 포멧</figcaption>
</figure>

<b>Thinker</b>
- 위의 CHatML 데이터셋 포멧 데이터를 사용해서 post-training 진행
- text 기반 대화 데이터, 음성 모달 대화 데이터, 오디오 모달 대화 데이터, 모달이 혼합된 데이터 사용

<b>Talker</b>
1. 맥락적 연속성(context continuation) 학습
    - In-Context Learning(ICL) 학습 단계에서 텍스트 supervised + 음성에서 다음 token 예측 수행
    - semantic한 특징을 음성에 monotonic하게 mapping
    - prosody, emotion, accent와 같은 맥락적에 적합한 다양한 특징을 생성할 수 있게 함
    - timbr를 음성에서 분리하는 기법도 사용해서 특정 목소리와 infrequent한 텍스트적 특성을 무시할 수 있게함

2. DPO로 음성 생성 안전성 확보
    - 이미지
    - reinforcement learning 단계를 도입하여 음성 생성에서의 안정성 확보
    - 데이터셋을 $(x, y_w, y_l)$로 구성
        - $x$: input 음성과 텍스트
        - $y_w$: 잘 생성된 음성
        - $y_l$: 잘못 생성된 음성
    - word-error rate(WER)과 puctuation pause error rate를 score로 사용해서 rank 진행

3. speaker fine-tuning 잔행하여, 특정한 목소리를 적용할 수 있게 하면서 자연스러움 향상

3. 음성 응담의 자연스러움과 통제성을 위해 multi-speaker instruction fine-tuning 진행

<!--# 🔍 개인적인 생각
 줄글로 작성 (내 해석/비판적 시각 강조)
예시:
이 논문은 제안한 방법으로 [문제 A]를 잘 해결하지만, [상황 X]에서는 성능 저하 가능성이 있습니다. 
또한 논문에서 다루지 않은 [측면 Y]에 대한 고려가 필요하다고 생각합니다. 
실제로 제가 관심 있는 [적용 분야]에서는 이러한 점을 주의해야 할 것 같습니다.
-->

<!--# 💡 내 연구/프로젝트에 어떻게 활용할까?
 줄글로 작성 (내 연구/프로젝트에의 적용 관점)
예시:
현재 진행 중인 [프로젝트명]에서는 이 논문의 [기법/아이디어]를 [부분/전체] 적용할 수 있을 것으로 보입니다. 
특히 [구체적 상황]에서는 큰 도움이 될 것 같고, 반면 [다른 상황]에서는 기존 방법과의 trade-off를 고려해야 할 것 같습니다.
-->

<!--# 📚 참고할 만한 연구 및 추가로 읽을 것
 Bullet 형태로 작성 (연결 논문, 추후 읽을 논문)
예시:
- [논문명1] - [간단한 관련성 설명]
- [논문명2] - [간단한 관련성 설명]
- Follow-up: [키워드/논문명3] 추가로 읽어볼 계획
-->

<!--# ✏️ 개인 메모 / 앞으로 할 일
 Bullet 형태로 작성 (개인 노트용, 자유롭게)
예시:
- [부분 A]는 아직 이해가 부족함 → 추가 복습 필요
- 실험 시 [파라미터 B]가 성능에 미치는 영향 확인해볼 것
- [팀원/멘토]와 이 논문에 대해 토론해보기
-->