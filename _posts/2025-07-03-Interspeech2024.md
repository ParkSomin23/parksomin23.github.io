---
title: "Interspeech 2024 ê´€ì‹¬ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸"
date: 2025-06-30 17:32
category: "ë…¼ë¬¸-ë¦¬ë·°"
tag: [Audio, SVC, VC, TTS]
published: true

toc: true
toc_sticky: true
use_math: true
---
> Interspeech 2024 ë…¼ë¬¸ë“¤ ì¤‘ ê´€ì‹¬ ìˆëŠ” ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
> [[Interspeech 2024 Archive](https://www.isca-archive.org/interspeech_2024/index.html)]

# ğŸ“‹ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
<!-- ê´€ì‹¬ ì •ë„: â­ ë‚®ìŒ / â­â­ ë³´í†µ / â­â­â­ ë†’ìŒ -->

<!-- 
**[ë…¼ë¬¸ì œëª©]() [[ì½”ë“œ]()] [[ë°ëª¨]()]**
- ì„¹ì…˜ëª…: ì„¹ì…˜ ì´ë¦„
- í‚¤ì›Œë“œ: [í‚¤ì›Œë“œ, ì˜ˆ: TTS, Emotion Recognition]
- ê´€ì‹¬ ì •ë„: â­â­â­
- ë©”ëª¨: [ê°„ë‹¨íˆ í¥ë¯¸ í¬ì¸íŠ¸ / ì™œ ì €ì¥í–ˆëŠ”ì§€] 

- **[ë…¼ë¬¸ì œëª©]() [[ì½”ë“œ]()] [[ë°ëª¨]()]**
    - ì„¹ì…˜ëª…: 
    - í‚¤ì›Œë“œ: 
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
-->
## Speech Features
- **[YOLOPitch: A Time-Frequency Dual-Branch YOLO Model for Pitch Estimation](https://www.isca-archive.org/interspeech_2024/li24ja_interspeech.html) [[ì½”ë“œ](https://github.com/xjuspeech/YOLOPitch)]**
    - ì„¹ì…˜ëª…: Speech and Audio Analysis and Representations
    - í‚¤ì›Œë“œ: F0 ì˜ˆì¸¡
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: F0 SOTA, F0ê°€ ìŒì„± ìƒì„±ì— ì£¼ìš”í•œ ìš”ì†Œë¡œ ì‚¬ìš©ë˜ëŠ” ê²½ìš°ê°€ ë§ê¸°ì— í™•ì¸í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì€ ë“¯í•¨ 

## VC: Voice Conversion
- **[Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals](https://www.isca-archive.org/interspeech_2024/seki24_interspeech.html) [[ì½”ë“œ](https://github.com/sarulab-speech/spatial_voice_conversion.git)] [[ë°ëª¨](https://sarulab-speech.github.io/demo_spatial_voice_conversion/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 1
    - í‚¤ì›Œë“œ: VC, ê³µê°„ ìŒí–¥
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - multi-speaker ìƒí™©ì—ì„œ ì›í•˜ëŠ” ëŒ€ìƒë§Œ Voice Conversion ê°€ëŠ¥
        - multi-channelë¡œ ê³µê°„ì  ìš”ì†Œ ê³ ë ¤
        - ë°ëª¨ì—ì„œ ê¸°ê³„ìŒì´ ì¡´ì¬í•¨

- **[Neural Codec Language Models for Disentangled and Textless Voice Conversion](https://www.isca-archive.org/interspeech_2024/baade24_interspeech.html) [[ì½”ë“œ (ë¯¸ì—…ë°ì´íŠ¸)](https://github.com/AlanBaade/DisentangledNCLM)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 1
    - í‚¤ì›Œë“œ: VC, neural codec language models
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - classifier free guidanceë¡œ ë°œí™”ì ìœ ì‚¬ë„ í–¥ìƒ
        - ê¸°ì¡´ codec language models ëŒ€ë¹„ ì—°ì‚°ëŸ‰ì´ ì ìŒ
        - accent disentanglementì™€ speaker similarity ì¢‹ìŒ
        - ë¹„êµ ë…¼ë¬¸ ì¤‘ [UniAudio](https://arxiv.org/abs/2310.00704)ë„ í™•ì¸í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŒ [[ì½”ë“œ](https://github.com/yangdongchao/UniAudio)]
        - ë°ëª¨ì™€ ì½”ë“œê°€ ì—†ì–´ì„œ ì•„ì‰¬ì›€

- **[Fine-Grained and Interpretable Neural Speech Editing](https://www.isca-archive.org/interspeech_2024/morrison24_interspeech.html) [[ì½”ë“œ](https://github.com/maxrmorrison/promonet)] [[ë°ëª¨](https://www.maxrmorrison.com/sites/promonet/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 1
    - í‚¤ì›Œë“œ: speech editing
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: 
        - speech editingì´ ì˜ë˜ê³  ìˆìŒ, pitch shifting/time strecthing ê°€ëŠ¥í•¨
        - VC í•­ëª©ì˜ ë‹¤ì–‘í•œ demoê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ì•˜ì„ ê²ƒ ê°™ìŒ

- **[FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation](https://www.isca-archive.org/interspeech_2024/kaneko24_interspeech.html) [[ë°ëª¨](https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/fastvoicegrad/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 1
    - í‚¤ì›Œë“œ: VC, diffusion
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: Diffusion step 1ë²ˆìœ¼ë¡œ ë³µì›

- **[DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion](https://www.isca-archive.org/interspeech_2024/ning24_interspeech.html) [[ë°ëª¨](https://nzqian.github.io/dualvc3/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 1
    - í‚¤ì›Œë“œ: VC, real-time
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨:  êµ‰ì¥íˆ ì§§ì€ ì˜¤ë””ì˜¤ë¡œë„ ìŒì„± ë³€í™˜ ê°€ëŠ¥

- **[Towards Realistic Emotional Voice Conversion using Controllable Emotional Intensity](https://www.isca-archive.org/interspeech_2024/qi24_interspeech.html) [[ë°ëª¨](https://jeremychee4.github.io/EINet4EVC/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 1
    - í‚¤ì›Œë“œ: VC, emotion
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: Emotional Intensity-aware, êµ¬ê¸€ ë…¼ë¬¸ì„ ë” ë¨¼ì € ì½ì–´ì•¼í•¨..!

- **[Utilizing Adaptive Global Response Normalization and Cluster-Based Pseudo Labels for Zero-Shot Voice Conversion](https://www.isca-archive.org/interspeech_2024/um24b_interspeech.html) [[ë°ëª¨](https://twiz0311.github.io/AGRN-VC/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: 
        - adaptive global response normalization(AGRN) ë°©ë²•ì´ í¥ë¯¸ë¡œì›Œ ë³´ì˜€ìŒ
        - íŠ¹íˆ ablation studies!

- **[Vec-Tok-VC+: Residual-enhanced Robust Zero-shot Voice Conversion with Progressive Constraints in a Dual-mode Training Strategy](https://www.isca-archive.org/interspeech_2024/ma24e_interspeech.html) [[ë°ëª¨](https://ma-linhan.github.io/VecTokVC-Plus/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC, cross-lingual, training-inference mismatch
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - ì„±ëŠ¥ì´ ë„ˆë¬´ ì¢‹ì•„ë³´ì´ëŠ”ë° ì½”ë“œê°€ ì—†ì–´ì„œ ë„ˆë¬´ ì•„ì‰¬ì›€..
        - 3ì´ˆ ìŒì„±ë§Œìœ¼ë¡œë„ VC ì§„í–‰ ê°€ëŠ¥
        - training-inference mismatch ë¬¸ì œ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²• ì œì•ˆ<br>
            "**teacher-guided refinement process** to form a dual-mode (conversion mode and reconstruction mode) training strategy with the original reconstruction process"

- **[Residual Speaker Representation for One-Shot Voice Conversion](https://www.isca-archive.org/interspeech_2024/xu24b_interspeech.html) [[ì½”ë“œ]()] [[ë°ëª¨](https://frostmiku.github.io/rsm/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC, timbre
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - timbre ì¡°ì ˆ ê°€ëŠ¥
        - ìƒˆë¡œìš´ speakerì— ëŒ€í•œ robustness ì¦ê°€
        - layer-wise error modeling ì‚¬ìš©í•´ì„œ ì„±ëŠ¥ í–¥ìƒ

- **[Disentangling prosody and timbre embeddings via voice conversion](https://www.isca-archive.org/interspeech_2024/gengembre24_interspeech.html)**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC, Disentangling, prosody, timbre
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: ìŒì„±ì„ prosodyì™€ timbreë¡œ ë¶„í•´, [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/docs/kr/README.ko.md) ì‚¬ìš©

- **[PRVAE-VC2: Non-Parallel Voice Conversion by Distillation of Speech Representations](https://www.isca-archive.org/interspeech_2024/tanaka24_interspeech.html) [[ë°ëª¨ (ì¬ìƒì´ ì•ˆ ë¨)](https://www.kecl.ntt.co.jp/people/tanaka.ko/projects/prvaevc2/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC. knowledge distillation
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: knowledge distillation

- **[HybridVC: Efficient Voice Style Conversion with Text and Audio Prompts](https://www.isca-archive.org/interspeech_2024/niu24_interspeech.html) [[ë°ëª¨](https://xinleiniu.github.io/HybridVC-demo/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC, text, prompt
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - supports text and audio prompts
        - ìŒìƒ‰ì´ ìœ ì‚¬ë„ëŠ” ì‚´ì§ ë–¨ì–´ì§€ëŠ” ë“¯í•˜ë‚˜ ì•„ì´ë””ì–´ê°€ ì¢‹ì•„ ë³´ì„


- **[DreamVoice: Text-Guided Voice Conversion](https://www.isca-archive.org/interspeech_2024/hai24_interspeech.html) [[HuggingFace](https://huggingface.co/myshell-ai/DreamVoice)] [[ë°ëª¨]( https://haidog-yaqub.github.io/dreamvoice_demo/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC, dataset, diffusion, text, prompt
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - text-guided VC
        - ë‹¤ë¥¸ ìŒì„± ë³€í™˜ ëª¨ë¸ì´ë‘ ë¶™í˜€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
        - ë°ì´í„°ì…‹ë„ ì œê³µ
        - Diffusion Probabilistic Models + Classifier-free Guidance

- **[Hear Your Face: Face-based voice conversion with F0 estimation]( https://www.isca-archive.org/interspeech_2024/lee24d_interspeech.html) [[ì½”ë“œ]()] [[ë°ëª¨](https://jaejunl.github.io/HYFace_Demo/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC, F0, pitch
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - ì–¼êµ´ ì‚¬ì§„ì„ ë³´ê³  ìŒì„± ë³€í™˜ì„ ìˆ˜í–‰í•¨..!
        - So-VITS-SVC ì•„ì´ë””ì–´ ë°œì „
        - ë°ëª¨ í€„ë¦¬í‹° ì¢‹ìŒ

- **[Knowledge Distillation from Self-Supervised Representation Learning Model with Discrete Speech Units for Any-to-Any Streaming Voice Conversion](https://www.isca-archive.org/interspeech_2024/kanagawa24b_interspeech.html) [[ë°ëª¨](https://hkanagawa.github.io/interspeech2024npvc/#unseen-to-unseen-conversion)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: VC, pitch, F0, voiced-unvoices, prosody, knowledge distillation, streaming
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - ë°ëª¨ ë¬¸ì¥ì´ ë‹¤ì–‘í•˜ì§€ ì•Šì•„ì„œ ì•„ì‰¬ì›€
        - ""The three dimensional prosody feature consists of z-scored log-F0 and **energy** and a binary voice-unvoiced flag"

## SVC: Singing Voice Conversion

- **[LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion with Singer Guidance](https://www.isca-archive.org/interspeech_2024/chen24e_interspeech.html) [[ë°ëª¨](https://sounddemos.github.io/ldm-svc/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 2
    - í‚¤ì›Œë“œ: SVC, diffusion
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: pre-trained So-VITS-SVC ì‚¬ìš©

- **[MakeSinger: A Semi-Supervised Training Method for Data-Efficient Singing Voice Synthesis via Classifier-free Diffusion Guidance](https://www.isca-archive.org/interspeech_2024/kim24i_interspeech.html) [[ë°ëª¨](https://makesinger.github.io/MakeSinger-demo/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Singing Voice Synthesis
    - í‚¤ì›Œë“œ: SVC, diffusion
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - speech ë°ì´í„°ì…‹ìœ¼ë¡œ ë…¸ë˜ ìŒì„± ë³€í™˜ì´ ê°€ëŠ¥í•¨

- **[Period Singer: Integrating Periodic and Aperiodic Variational Autoencoders for Natural-Sounding End-to-End Singing Voice Synthesis](https://www.isca-archive.org/interspeech_2024/kim24p_interspeech.html) [[ë°ëª¨](https://rlataewoo.github.io/periodsinger/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Singing Voice Synthesis
    - í‚¤ì›Œë“œ: SVC, pitch
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - í€„ë¦¬í‹°ê°€ ì¢‹ìŒ!
        - VITS + music score
        - ë¹„êµí•œ ë‹¤ë¥¸ ë…¼ë¬¸ì¸ [VISinger2](https://arxiv.org/abs/2211.02903)([ì½”ë“œ](https://github.com/zhangyongmao/VISinger2))ë„ ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ìŒ
        - owing to deterministic pitch conditioning, they do not fully address the one-to-many problem.
            - integrates variational autoencoders for the periodic and aperiodic components
            - eliminates the dependency on an external aligner by estimating the phoneme alignment through a monotonic alignment search within note boundaries.
        - pitch augmentation
            - we apply the smoothed pitch augmentation method to ensure that the latent variables capture both wide and narrow pitch variations
            - For pitch augmentation, we extracted the smoothed F0 using a median filter with a kernel size of 13.

- **[X-Singer: Code-Mixed Singing Voice Synthesis via Cross-Lingual Learning](https://www.isca-archive.org/interspeech_2024/hwang24_interspeech.html) [[ë°ëª¨]()]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Singing Voice Synthesis
    - í‚¤ì›Œë“œ: SVC, multi-lingual
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - ë„· ë§ˆ~ ë¸”!
        - í•˜ë‚˜ì˜ ìƒ˜í”Œ ì•ˆì— ì¼ë³¸ì–´, ì¤‘êµ­ì–´, í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë°”ê¾¸ë©´ì„œ ë…¸ë˜ ìƒì„± ê°€ëŠ¥
        - languageì™€ speaker ë¶„ë¦¬: mix-LN transformer (Mix-LN mixes the feature statistics of the speaker embedding, which confuses the model by the mismatched speaker information)
        - CFM-based decoder
            - conditional flow matching ì‚¬ìš© (matchaTTS, P-Flow)
        - ëª…ì‹œì ì¸ pitch ì˜ˆì¸¡ ì—†ì´ë„ ë¹„ìŠ·í•œ ëª©ì†Œë¦¬ë¡œ ìƒì„± ê°€ëŠ¥
        - zero-shot/ë” í’ë¶€í•œ í‘œí˜„/ë°œí™” ë°ì´í„° ì‚¬ìš©ì— ëŒ€í•œ ì—°êµ¬ëŠ” ì¶”í›„ì— ì§„í–‰ëœë‹¤ê³  í•¨

## TTS & Speech Synthesis
- **[Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model](https://www.isca-archive.org/interspeech_2024/xue24c_interspeech.html) [[ë°ëª¨](https://happylittlecat2333.github.io/interspeech2024)]**
    - ì„¹ì…˜ëª…: Zero-shot TTS
    - í‚¤ì›Œë“œ: TTS, LLM, emotion
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - source ì˜¤ë””ì˜¤ì˜ ê°ì •ì„ ë°˜ì˜í•¨!
        - pretrained LLM ì‚¬ìš©í•´ì„œ semantic token ì„±ëŠ¥ í–¥ìƒ ì‹œí‚´
        - í›‘ì–´ë³´ëŠ” ê±´ í•„ìˆ˜

- **[DINO-VITS: Data-Efficient Zero-Shot TTS with Self-Supervised Speaker Verification Loss for Noise Robustness](https://www.isca-archive.org/interspeech_2024/pankov24_interspeech.html)**
    - ì„¹ì…˜ëª…: Zero-shot TTS
    - í‚¤ì›Œë“œ: TTS, HuBERT, teacher-student EMA model, noise augmentation
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - DINO lossë¥¼ ì‚¬ìš©í•´ì„œ voice cloning ì„±ëŠ¥ í–¥ìƒ<br>
        "substantial improvements in naturalness and speaker similarity in both clean and especially real-life noisy scenarios, outperforming traditional AAM-Softmax-based training methods"
        - HuBERTë¥¼ ì‚¬ìš©í•˜ë©´ ë…¸ì´ì¦ˆ ìœ ë¬´ê°€ í¬í•¨ëœ embeddingì„ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë…¸ì´ì¦ˆ label ì—†ì´ë„ ë…¸ì´ì¦ˆ ìˆëŠ” ë°ì´í„° í•™ìŠµ ê°€ëŠ¥
        - pretrained speaker verification CAM++ model ì‚¬ìš©

- **[Unsupervised Domain Adaptation for Speech Emotion Recognition using K-Nearest Neighbors Voice Conversion](https://www.isca-archive.org/interspeech_2024/mote24_interspeech.html)**
    - ì„¹ì…˜ëª…: Corpora-based Approaches in Automatic Emotion Recognition
    - í‚¤ì›Œë“œ: emotion, domain adaptation
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - binì„ ì‚¬ìš©í•œ ë°©ë²•ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ë³´ê³ ì í•¨
            - ë‚˜ëŠ” ì¡¸ì—… ë…¼ë¬¸ì—ì„œ binì„ 3ê°œë¡œ ë‚˜ëˆ„ì—ˆëŠ”ë°, í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” 5ê°œ, 10ê°œë¡œ ë‚˜ëˆ´ìŒ
        - ê¸°ë°˜ ë…¼ë¬¸ë„ í™•ì¸ì´ í•„ìš”í•´ë³´ì„<br>
        "We implement our idea using the K-nearest neighbors-voice conversion strategy [[19](https://arxiv.org/abs/2305.18975)], which is a recently proposed approach that achieves impressive results in VC despite its simplicity"

- **[GTR-Voice: Articulatory Phonetics Informed Controllable Expressive Speech Synthesis](https://www.isca-archive.org/interspeech_2024/li24pa_interspeech.html) [[ë°ëª¨](https://demo.gtr-voice.com/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Expressivity and Emotion
    - í‚¤ì›Œë“œ: TTS, emotion, dataset
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - Glottalization, Tenseness, and Resonance label ì‚¬ìš©
            - Glottalization: control of air flow due to the tension of the glottis (i.e., throat)
            - Tenseness: tense vowels in pronunciation involve tension in the tip and root of the tongue, while lax vowels are the opposite.
            - Resonance: integration of articulatory phonetics with vocal register insight (í‰ì„±, ë‘ì„±ìœ¼ë¡œ ì¶”ì²­ë¨)

- **[TSP-TTS: Text-based Style Predictor with Residual Vector Quantization for Expressive Text-to-Speech](https://www.isca-archive.org/interspeech_2024/seong24b_interspeech.html) [[ë°ëª¨](https://seongdonghyun.github.io/TSP-TTS-DEMO/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Expressivity and Emotion
    - í‚¤ì›Œë“œ: TTS, expressive, text, prompt
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - reference ìŒì„± ì—†ì´ text-baseë¡œ ë°œí™” ìŠ¤íƒ€ì¼ ì¶”ì¶œ
        - ë°ëª¨ì— í•œêµ­ì–´ ìˆìŒ
        - unseen speakerì— ëŒ€í•´ì„œëŠ” ê°ì • í‘œí˜„ì´ ì•½í•´ì§€ëŠ” ê²ƒ ê°™ìŒ (í•™ìŠµëœ í™”ìê°€ 4ëª…ì´ë¼ì„œ ì–´ì©” ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì¸ ê²ƒ ê°™ì•„ ë³´ì„. ë” ë§ì€ í™”ìë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ê²°ê³¼ê°€ ê¶ê¸ˆí•¨!)
        - 2080 Ti 2ì¥ì˜ ê²°ê³¼ë¬¼ì´ë€ ê²Œ ë„ˆë¬´ ëŒ€ë‹¨í•¨.. 


- **[Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous Behaviors Based on Language Models](https://www.isca-archive.org/interspeech_2024/li24na_interspeech.html) [[ë°ëª¨](https://thuhcsi.github.io/interspeech2024-SponLMTTS/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Expressivity and Emotion
    - í‚¤ì›Œë“œ: TTS, expressive, LM
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: 
        - â€œìŒ~â€ê³¼ ì›ƒìŒ ì†Œë¦¬ë¥¼ ë‹¤ì–‘í•œ ë²„ì „ìœ¼ë¡œ í•©ì„±í•  ìˆ˜ ìˆìŒ
        - LM ê¸°ë°˜ TTS ëª¨ë¸ì´ê³ , acoustic decoderëŠ” VALL-E ê¸°ë°˜

- **[Text-aware and Context-aware Expressive Audiobook Speech Synthesis](https://www.isca-archive.org/interspeech_2024/guo24d_interspeech.html) [[ë°ëª¨](https://dukguo.github.io/TACA-TTS/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Expressivity and Emotion
    - í‚¤ì›Œë“œ: TTS, emotion, LM, text
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: 
        - textë¿ë§Œ ì•„ë‹ˆë¼ ë§¥ë½ê¹Œì§€ ê³ ë ¤í•œ ëª¨ë¸
        - ë‹¤ë¥¸ ëª¨ë¸ ëŒ€ë¹„ ëœ ë”±ë”±í•˜ê²Œ ì½ëŠ” ëŠë‚Œì´ ìˆìŒ (ë°ëª¨ê°€ ì¤‘êµ­ì–´ë¼ì„œ ë“£ëŠ”ë° í•œê³„ê°€ ìˆìŒ)

- **[Controlling Emotion in Text-to-Speech with Natural Language Prompts](https://www.isca-archive.org/interspeech_2024/bott24_interspeech.html) [[toolkit](https://github.com/DigitalPhonetics/IMS-Toucan/tree/ToucanTTS_Prompting)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Expressivity and Emotion
    - í‚¤ì›Œë“œ: TTS, emotion, text, prompt
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - ê°ì •ì ì¸ ìš”ì†Œê°€ ìˆëŠ” textë¥¼ promptë¡œ ì‚¬ìš© (ex: (ì¤‘ë¦½)ì•Œê² ìŠµë‹ˆë‹¤ / (í–‰ë³µ)ì •ë§ìš”! )
        - contribution
            1. an architecture that allows for separate modeling of a speakerâ€™s voice and
            the prosody of an utterance, using a natural language prompt for the latter
            2. a training strategy to learn a strongly generalized prompt conditioning
            3. a pipeline that allows users to generate speech with fitting prosody without manually selecting the emotion by simply using the text to be read as the prompt

- **[Emotion Arithmetic: Emotional Speech Synthesis via Weight Space Interpolation](https://www.isca-archive.org/interspeech_2024/kalyan24_interspeech.html) [[ë°ëª¨](https://tinyurl.com/hnmz987n)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Expressivity and Emotion
    - í‚¤ì›Œë“œ: TTS, emotion
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: ê° ê°ì •ìœ¼ë¡œ fine-tuningí•œ ëª¨ë¸ê³¼ base modelì˜ ì°¨ì´ë¥¼ emotion vectorë¡œ ì‚¬ìš©

- **[EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech](https://www.isca-archive.org/interspeech_2024/cho24_interspeech.html) [[ë°ëª¨](https://EmoSphere-TTS.github.io/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Expressivity and Emotion
    - í‚¤ì›Œë“œ: TTS, emotion
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: ì„ì‚¬ ë•Œ emotion sphereì™€ ê°™ì´ í•´ë³´ê³  ì‹¶ì—ˆëŠ”ë°, í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ë°©ë²•ì„ ì œì‹œí•´ì„œ ê¶ê¸ˆí•¨!


- **[Word-level Text Markup for Prosody Control in Speech Synthesis](https://www.isca-archive.org/interspeech_2024/korotkova24_interspeech.html) [[ì½”ë“œ](https://github.com/just-ai/speechflow)] [[ë°ëª¨](https://yuliya1324.github.io/prosody_control_TTS/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Prosody
    - í‚¤ì›Œë“œ: TTS, prosody
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: prosodic markup - prosodyë¥¼ unsupervise ë°©ë²•ìœ¼ë¡œ í•™ìŠµí•˜ê³ , control í•  ìˆ˜ ìˆê²Œí•œ ë…¼ë¬¸


- **[Should you use a probabilistic duration model in TTS? Probably! Especially for spontaneous speech](https://www.isca-archive.org/interspeech_2024/mehta24b_interspeech.html)[[ë°ëª¨](https://shivammehta25.github.io/prob_dur/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Prosody
    - í‚¤ì›Œë“œ: TTS, prosody
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: 
        - ê¸°ì¡´ nonautoregressive TTSì˜ deterministic duration predictor(DET)ì„ probabilistic duration modelling(OT-CFM-based duration model, FM)ë¡œ ë°”ê¾¸ê³  ë¹„êµ<br>
        "We explore the effects of replacing the MSE-based duration predictor in existing NAR TTS approaches with a log-domain duration model based on conditional flow matching"
        - ë¹„êµì— ì‚¬ìš©í•œ ë…¼ë¬¸
            - a deterministic acoustic model (FastSpeech 2)
            - an advanced deep generative acoustic model (Matcha-TTS)
            - a probabilistic endto-end TTS model (VITS)

- **[Total-Duration-Aware Duration Modeling for Text-to-Speech Systems](https://www.isca-archive.org/interspeech_2024/eskimez24_interspeech.html)**
    - ì„¹ì…˜ëª…: Speech Synthesis: Prosody
    - í‚¤ì›Œë“œ: TTS, prosody, duration
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - "designed to precisely control the length of generated speech while maintaining speech quality at different speech rates"
        - "a novel duration model based on Mask"GIT-based to enhance the diversity and quality of the phoneme durations"

- **[Towards Expressive Zero-Shot Speech Synthesis with Hierarchical Prosody Modeling]( https://www.isca-archive.org/interspeech_2024/jiang24d_interspeech.html) [[ë°ëª¨](https://www.isca-archive.org/interspeech_2024/jiang24d_interspeech.pdf)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Prosody
    - í‚¤ì›Œë“œ: TTS, prosody, diffusion
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: 
        - ì–µì–‘ì´ ì•ˆ ë‹®ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë…¼ë¬¸
        - contribution
            1.speaker timbre is a global attribute: speaker encoder to extract global speaker embedding (input: mel spectrograms)
            2.diffusion model as a pitch predictor: to match speech prosody diversity by leveraging its natural advantage in generating content diversity
            3. prosody shows both global consistency and local variations: to model prosody hierarchically, such as frame-level, phoneme level, and word-level, to improve the prosody performance of synthesized speech. 

- **[Low-dimensional Style Token Control for Hyperarticulated Speech Synthesis](https://www.isca-archive.org/interspeech_2024/nishihara24_interspeech.html) [[ë°ëª¨](https://groups.inf.ed.ac.uk/cstr3/sgile/gst/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 1
    - í‚¤ì›Œë“œ: TTS
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ëŠ” ê²ƒê³¼ ë˜ë°•ë˜ë°• ë§í•˜ëŠ” ìŠ¤íƒ€ì¼ ì„ íƒ ê°€ëŠ¥
        - ì•„ì´ë””ì–´ ë¶€ë¶„ì„ ë” ìì„¸íˆ ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŒ

- **[Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation](https://www.isca-archive.org/interspeech_2024/li24ba_interspeech.html) [[ë°ëª¨](https://kkksuper.github.io/Single-Codec)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 1
    - í‚¤ì›Œë“œ: TTS, codec
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: 
        - single-codebook codec, compression and reconstruction on mel-spectrogram
        - "Single-Codec performs compression and reconstruction on Mel Spectrogram instead of the raw waveform, enabling efficient compression of speech information while preserving important details, as stated in Tortoise-TTS"

- **[ClariTTS: Feature-ratio Normalization and Duration Stabilization for Code-mixed Multi-speaker Speech Synthesis](https://www.isca-archive.org/interspeech_2024/kim24h_interspeech.html) [[ë°ëª¨](https://claritts.github.io/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 1
    - í‚¤ì›Œë“œ: TTS, cross-lingual, code-switching
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - í˜„ëŒ€ ìë™ì°¨ì—ì„œ ë§Œë“¦
        - í•œ ë¬¸ì¥ ë‚´ì—ì„œ ì˜ì–´ì™€ í•œêµ­ì–´ code-switching ê°€ëŠ¥ (cross-lingual and code-mixed speech with high naturalness), í•´ë‹¹ ë¶€ë¶„ì˜ ì•„ì´ë””ì–´ë¥¼ ìì„¸íˆ ë³¼ í•„ìš” ìˆìŒ

- **[Multi-modal Adversarial Training for Zero-Shot Voice Cloning](https://www.isca-archive.org/interspeech_2024/janiczek24_interspeech.html)**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 1
    - í‚¤ì›Œë“œ: TTS
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - Zoom~
        - "GAN-based, FastSpeech2 acoustic model and training on Libriheavy, a large multi-speaker dataset, for the task of zeroshot voice cloning"
        - "Multi-feature Generative Adversarial Training pipeline which uses our discriminator to enhance both acoustic and prosodic features for natural and expressive TTS"

- **[Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning](https://www.isca-archive.org/interspeech_2024/chien24b_interspeech.html)**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 1
    - í‚¤ì›Œë“œ: TTS, markup, expressive
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - pre-trained Voicebox ì‚¬ìš©í•´ì„œ ì•„ë˜ 3ê°€ì§€ ê²½ìš° controlí•œ ìŒì„± ìƒì„±
            - Punctuation: It's good!
            - Emphasis:It's *good*
            - Laughter: It's good [laughter]
        - "efficient fine-tuning methods to bridge the gap between pre-trained parameters and new fine-grained conditioning modules"

- **[Lina-Speech: Gated Linear Attention is a Fast and Parameter-Efficient Learner for text-to-speech synthesis](https://www.isca-archive.org/interspeech_2024/lemerle24_interspeech.html) [[ì½”ë“œ](https://github.com/theodorblackbird/lina-speech)] [[ë°ëª¨](https://theodorblackbird.github.io/blog/demo_lina/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 2
    - í‚¤ì›Œë“œ: TTS, 
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - neural codec language model<br>
        "In contrast with previous TTS codec LM model that leverages decoder-only (GPT) transformers, Small-E relies on encoder-decoder architecture"
        - **Can be easily pretrained and finetuned on midrange GPUs**
        - Trained on long context
        

- **[Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment](https://www.isca-archive.org/interspeech_2024/neekhara24_interspeech.html) [[ë°ëª¨](https://t5tts.github.io/)] [[nvidia blog](https://developer.nvidia.com/ko-kr/blog/addressing-hallucinations-in-speech-synthesis-llms-with-the-nvidia-nemo-t5-tts-model/?linkId=100000273031741)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 2
    - í‚¤ì›Œë“œ: TTS, duration, LLM
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - nvidia, T5-TTS (T5: text-to-text model)
        - "first attampt at synthesizing multi-codebook neural audio codecs with an encoder-decoder architecture"
        - cross-attention headsê°€ monotonic alignmentë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë§Œë“¦
        - ì—°ì†ìœ¼ë¡œ ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë‚˜ ë¬¸ì¥ì— ëŒ€í•´ì„œ ì—„ì²­ ìì—°ìŠ¤ëŸ½ê²Œ ë°œí™”í•¨

- **[Synthesizing Long-Form Speech merely from Sentence-Level Corpus with Content Extrapolation and LLM Contextual Enrichment](https://www.isca-archive.org/interspeech_2024/lai24b_interspeech.html) [[ë°ëª¨(ì•ˆëœ¸)](https://speechpaper.github.io/is2024/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 2
    - í‚¤ì›Œë“œ: TTS, 
    - ê´€ì‹¬ ì •ë„: â­â­
    - ë©”ëª¨: sentence ë‹¨ìœ„ì˜ ìŒì„±ë§Œìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ longform speech ìƒì„± ê°€ëŠ¥


- **[ë…¼ë¬¸ì œëª©](https://www.isca-archive.org/interspeech_2024/liu24p_interspeech.html) [[ì½”ë“œ](https://github.com/AI-S2-Lab/FluentEditor)] [[ë°ëª¨](https://ai-s2-lab.github.io/FluentEditor/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 2
    - í‚¤ì›Œë“œ: TTS, text, speech editing
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - Text-based Speech Editing
        - Acoustic and Prosody Consistency Losses
            - Acoustic: quantify the smooth transition between the editing region and the adjacent context
            - Prosody: for capturing the prosody feature from the predicted masked region while also analyzing the overall prosody characteristics present in the original speech

- **[High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model](https://www.isca-archive.org/interspeech_2024/lee24f_interspeech.html) [[ë°ëª¨](https://srtts.github.io/interpreting-speaking/)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 2
    - í‚¤ì›Œë“œ: TTS, text
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - í€„ë¦¬í‹° ì—„ì²­ ì¢‹ê³ , controlí•œ ì˜¤ë””ì˜¤ë„ ìì—°ìŠ¤ëŸ¬ì›€
        - Interpreting: text-to-semantic token stage
            - k-means clustering on wav2vec 2.0
            - mainly focus on phonetic information, but it also dealing with some prosodic information such as speech rate and overall pitch contour.
        - Speaking: semantic to the acoustic token stage (HiFi-Codec)

- **[ë…¼ë¬¸ì œëª©](https://www.isca-archive.org/interspeech_2024/lenglet24_interspeech.html) [[ì½”ë“œ](https://github.com/MartinLenglet/FastLips)] [[ë°ëª¨](https://ssw2023.org/demo/FastLips/index.html)]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Paradigms and Methods 2
    - í‚¤ì›Œë“œ: TTS, vision, text
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - "generate speech and co-verbal facial movements from text, animating a virtual avatar"
        - "The proposed model generates mel-spectrograms and facial features (head, eyes, jaw and lip movements) to drive the virtual avatarâ€™s action units"

## Speech Emotion Recognition
- **[An Effective Local Prototypical Mapping Network for Speech Emotion Recognition](https://www.isca-archive.org/interspeech_2024/xi24_interspeech.html)**
    - ì„¹ì…˜ëª…: Corpora-based Approaches in Automatic Emotion Recognition
    - í‚¤ì›Œë“œ: emotion, Prototype selection
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - IEMOCAP accuracy: 77.42%(WA), 75.82%(UA) ë‹¬ì„±
        - MIL ê¸°ë°˜ Prototype selection ê¸°ë²• ì œì•ˆ
        - ì„ì‚¬ ë•Œ, MILì„ ì‚¬ìš©í•´ë³´ë ¤ê³  í–ˆë˜ ì…ì¥ìœ¼ë¡œ ê¼­ ì½ì–´ë³´ê³  ì‹¶ìŒ


- **[Speech Emotion Recognition with Multi-level Acoustic and Semantic Information Extraction and Interaction](https://www.isca-archive.org/interspeech_2024/gao24f_interspeech.html)**
    - ì„¹ì…˜ëª…: Corpora-based Approaches in Automatic Emotion Recognition
    - í‚¤ì›Œë“œ: emotion, joint training
    - ê´€ì‹¬ ì •ë„: â­â­â­
    - ë©”ëª¨: 
        - IEMOCAP accuracy: 79.50%(WA), 79.62%(UA) ë‹¬ì„±
        - ASR, SERë¥¼ ê°ê° í•™ìŠµí•œ í›„, joint trainingì„ ì§„í–‰í•˜ëŠ” ë°©ë²• ì‚¬ìš©
        - SERì— textë„ ì¤‘ìš”í•œ ìš”ì†Œì„ì„ ë³´ì—¬ì¤Œ

## Audio Captioning
- **[Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding](https://www.isca-archive.org/interspeech_2024/liu24_interspeech.html) [[ì½”ë“œ](https://github.com/frankenliu/LOAE)]**
    - ì„¹ì…˜ëª…: Audio Captioning, Tagging, and Audio-Text Retrieval
    - í‚¤ì›Œë“œ: 
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: outperforms the winner of DCASE 2023 Task 6A on almost all metrics.

- **[Streaming Audio Transformers for Online Audio Tagging](https://www.isca-archive.org/interspeech_2024/dinkel24_interspeech.html) [[ì½”ë“œ](https://github.com/RicherMans/SAT)]**
    - ì„¹ì…˜ëª…: Audio Captioning, Tagging, and Audio-Text Retrieval
    - í‚¤ì›Œë“œ: straming
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 2ì´ˆ ì •ë„ ì‹œê°„ ì†Œìš”<br>
    "The best model, SAT-B, achieves an mAP of 45.1 with a 2s delay, using 8.2 Gflops and 36 MB of memory during inference.""

- **[Efficient CNNs with Quaternion Transformations and Pruning for Audio Tagging](https://www.isca-archive.org/interspeech_2024/chaudhary24_interspeech.html) [[ì½”ë“œ](https://github.com/Cross-Caps/QPANN)]**
    - ì„¹ì…˜ëª…: Audio Captioning, Tagging, and Audio-Text Retrieval
    - í‚¤ì›Œë“œ: 
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 

- **[ParaCLAP â€“ Towards a general language-audio model for computational paralinguistic tasks](https://www.isca-archive.org/interspeech_2024/jing24b_interspeech.html) [[ì½”ë“œ](https://github.com/KeiKinn/ParaCLAP)]**
    - ì„¹ì…˜ëª…: Audio Captioning, Tagging, and Audio-Text Retrieval
    - í‚¤ì›Œë“œ: 
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: SER taskì—ì„œ ì‹œì‘í•´ì„œ ë°œì „ë¨(EMOTION, VAD, GENDER), "surpass the performance of open-source state-of-the-art models"

## Etc
- **[Universal Score-based Speech Enhancement with High Content Preservation](https://www.isca-archive.org/interspeech_2024/scheibler24_interspeech.html) [[ì½”ë“œ](https://github.com/line/open-universe?tab=readme-ov-file)]**
    - ì„¹ì…˜ëª…: Generative Speech Enhancement
    - í‚¤ì›Œë“œ: 
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 

- **[SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models](https://www.isca-archive.org/interspeech_2024/yin24b_interspeech.html)**
    - ì„¹ì…˜ëª…: Speech Synthesis: Evaluation
    - í‚¤ì›Œë“œ: VC, evaluation
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 
        - VC speaker similarity í‰ê°€
        - ì„ í–‰ ë…¼ë¬¸ ì½”ë“œ: [SVSNet: An end-to-end speaker voice similarity assessment model](https://github.com/n1243645679976/SVSNet)

- **[LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning](https://www.isca-archive.org/interspeech_2024/kawamura24_interspeech.html) [[ì½”ë“œ]()] [[ë°ëª¨]()]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Tools and Data
    - í‚¤ì›Œë“œ: 
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: voice taggingì— ì“°ê¸° ì¢‹ì„ ê²ƒ ê°™ìŒ
    
- **[Towards Naturalistic Voice Conversion: NaturalVoices Dataset with an Automatic Processing Pipeline](https://www.isca-archive.org/interspeech_2024/salman24_interspeech.html) [[ì½”ë“œ]()] [[ë°ëª¨]()]**
    - ì„¹ì…˜ëª…: Speech Synthesis: Voice Conversion 3
    - í‚¤ì›Œë“œ: VC, dataset
    - ê´€ì‹¬ ì •ë„: â­â­
    <figure class="centered-figure">
    <img src="../assets/images/interspeech2024/image5.png"  style="width: 80%;">
    </figure>

- **[VoxSim: A perceptual voice similarity dataset](https://www.isca-archive.org/interspeech_2024/ahn24b_interspeech.html)**
    - ì„¹ì…˜ëª…: Oth
    - í‚¤ì›Œë“œ: dataset, speaker similarity
    - ê´€ì‹¬ ì •ë„: â­
    - ë©”ëª¨: 41k utterance pairs from the VoxCeleb dataset, collect 70k speaker similarity scores through a listening test
    <figure class="centered-figure">
    <img src="../assets/images/interspeech2024/image1.png"  style="width: 60%;">
    </figure>

- **[SAMSEMO: New dataset for multilingual and multimodal emotion recognition](https://www.isca-archive.org/interspeech_2024/bujnowski24_interspeech.html) [[ì½”ë“œ](https://github.com/samsungnlp/samsemo)]**
    - ì„¹ì…˜ëª…: Oth
    - í‚¤ì›Œë“œ: dataset, multi-lingual, emotion
    - ê´€ì‹¬ ì •ë„: â­â­
    <figure class="centered-figure">
    <img src="../assets/images/interspeech2024/image2.png"  style="width: 70%;">
    </figure>


